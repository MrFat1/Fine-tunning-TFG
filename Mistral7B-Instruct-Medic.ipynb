{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a6f583-2990-41af-a203-ce7745438c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q git+https://github.com/huggingface/peft.git\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q trl\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q wandb -U\n",
    "!pip install -q -U matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b74619e4-24f4-440a-bc96-a9fc810c5c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n",
      "\n",
      "- `transformers` version: 4.40.0\n",
      "- Platform: Windows-10-10.0.22621-SP0\n",
      "- Python version: 3.11.9\n",
      "- Huggingface_hub version: 0.22.2\n",
      "- Safetensors version: 0.4.3\n",
      "- Accelerate version: 0.29.3\n",
      "- Accelerate config: \tnot found\n",
      "- PyTorch version (GPU?): 2.2.2+cu121 (True)\n",
      "- Tensorflow version (GPU?): not installed (NA)\n",
      "- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n",
      "- Jax version: not installed\n",
      "- JaxLib version: not installed\n",
      "- Using GPU in script?: <fill in>\n",
      "- Using distributed or parallel set-up in script?: <fill in>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!transformers-cli env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8d8796-5f4d-4aaa-9627-d029d2dd23b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Aug_15_22:09:35_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.140\n",
      "Build cuda_12.2.r12.2/compiler.33191640_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "394c68ab-9bb8-4094-a192-cb806492d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipywidgets\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "223b6592-c4ff-4ea5-945f-77f26886a478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4619387f3345d58f4256a929ccd8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Log in to the HugginFace Model Hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dcd0ceb-679e-4912-ad4c-058dfe4dacbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: unrecognized arguments: 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\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ebe4c-6fd8-4e6c-864a-8eba0cb31f60",
   "metadata": {},
   "source": [
    "## Prueba del modelo base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dd1aa6e-9ccc-4a2a-8bb1-7133967827a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import notebook_login # Usaremos las herramientas de HuggingFace para el entrenamiento\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import accelerate\n",
    "import tensorboardX\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "273d8210-03da-47cd-8a09-89226167022d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\201902452\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\201902452\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Base Model\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load MitsralAi tokenizer for dataset formatting\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.padding_side = \"left\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd2caf-37b0-4bb7-975a-8c0f9128e84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3afd44-8c39-4d4d-a730-f029ded9d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros del modelo\n",
    "\n",
    "# Final model name\n",
    "tuned_model = \"mistral7b_medic\"\n",
    "\n",
    "######### QLORA Params #############\n",
    "# (Para reducir el uso de memoria)\n",
    "\n",
    "# Estos valores dependen del dataset\n",
    "# The rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. \n",
    "# A higher rank will allow for more expressivity, but there is a compute tradeoff. (2^x)\n",
    "lora_r = 32\n",
    "\n",
    "# Scaling factor for the learned weights. The weight matrix is scaled by alpha/r, and thus a higher value for alpha assigns more weight to \n",
    "# the LoRA activations.\n",
    "lora_alpha = 64\n",
    "\n",
    "# NOTA: En el paper de QLoRA utiliza los valoes de r = 64 y alpha = 16, argumentando que estos valores generalizan bastante bien. Si queremos darle mas\n",
    "# importancia a la data fine-tuneada aumentamos los valores de alpha y si queremos mejor rendimiento disminuimos R.\n",
    "\n",
    "# Dropout probability\n",
    "# Durante el entranmiento, en cada epoch hay un {lora_dropout}% de que las neuronas se desactiven (para que trabaje mas)\n",
    "lora_dropout = 0.1\n",
    "\n",
    "####### BitsAndBytes param ###########\n",
    "\n",
    "#Activamos la reducción de precisión a 4-bit\n",
    "use_4bit = True\n",
    "\n",
    "# Parámetro para los modelos 4-bit\n",
    "bnb_4bit_compute_dtype = \"bfloat16\" #torch.float16 != torch.bfloat16\n",
    "\n",
    "# Tipo de cuantización (fp4 o nf4)\n",
    "# nf4 utiliza una distribución normal\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Nested quantization for 4-bit base models (double quantization)\n",
    "# Nos proporciona una mayor eficiencia de memoria sin sacrificiar rendimiento. Lo que hace es\n",
    "# realizar una segunda cuantización de los pesos ya cuantizados para ahorrar 0.4 bits/parametro.\n",
    "use_nested_quant = True\n",
    "\n",
    "####### Training Arguments param #########\n",
    "\n",
    "#Aqui se guardarán las predicciones y los checkpoints\n",
    "output_dir = \"./resultados\"\n",
    "\n",
    "# Número de epochs (Iteraciones por todo el dataset)\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for train\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 2\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2.5e-5\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "# The total number of training steps to perform.\n",
    "# Cantidad de batches que pasamos por el modelo\n",
    "# Uso: Inicialmente a muchos steps y comprobamos a partir de que steps el modelo empieza a degradarse. Para evitar hacer muchos entrenamientos\n",
    "# en la proxima iteración empezamos desde un checkpoint.\n",
    "max_steps = 250\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 1\n",
    "\n",
    "###### Parámetros para SFT ########\n",
    "\n",
    "# Max sequence length\n",
    "max_seq_length = 512\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "# (En el código de Accelerate esta todo explicado)\n",
    "# device_map = {\"\": 0}\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a51365b6-a74a-403e-a8ef-d73f23ce6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load base model\n",
    "\n",
    "# Load with QLoRA config\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# https://huggingface.co/docs/transformers/main_classes/quantization\n",
    "\n",
    "# Con la librería Transformers podemos usar los algoritmos AWQ y GPTQ de cuantización y soporta\n",
    "# cuantizaciones de 4 y 8 bits. (Se pueden añadir más técnicas con la clase HfQuantizer)\n",
    "# En este caso cuantizaremos a 4-bit con el tipo NF4\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b0564fd-8b00-4b9a-a04c-f22b29e008d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|████████████████████████████████████████████████████████████████| 3/3 [02:06<00:00, 42.31s/it]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/model_doc/auto\n",
    "# https://huggingface.co/transformers/v2.9.1/main_classes/model.html\n",
    "\n",
    "# Utilizamos la arquitectura que viene ya incluida en el modelo\n",
    "# Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage = True,\n",
    "    device_map = device_map\n",
    ")\n",
    "\n",
    "# https://huggingface.co/transformers/v2.9.1/main_classes/model.html#transformers.PreTrainedModel.generate\n",
    "#Use past key values?\n",
    "base_model.config.use_cache = False  # Nos interesa usar los parametros actualizados, no los viejos (cached)\n",
    "\n",
    "# Mimic the behaviour of the original model at inference?\n",
    "base_model.config.pretraining_tp = 1 #1 = disable\n",
    "\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23888e13-7c19-4ed7-ba96-2fd2c788be99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Molluscum contagiosum?\n",
      "\n",
      "Molluscum contagiosum is a common viral infection that causes small, raised, pearly or flesh-colored bumps on the skin. These bumps, which can be itchy or painless, can appear anywhere on the body but are most commonly found on the face, arms, and legs. The bumps have a small central indentation or dimple and can be round, oval, or dome-shaped. They can range in size from a few millimeters to several centimeters in diameter.\n",
      "\n",
      "Molluscum contagiosum is highly contagious and can be spread through direct skin-to-skin contact or by touching contaminated objects such as towels, clothing, or toys. The virus can survive on surfaces for long periods of time, increasing the risk of transmission.\n",
      "\n",
      "The condition is more common in children and young adults, and it can affect people of all ages and backgrounds. While the bumps can be unsightly and embarrassing, they are generally harmless and usually clear up on their own within 6 to 12 months. However, some people may choose to seek treatment to speed up the healing process or to reduce the risk of spreading the infection to others.\n",
      "\n",
      "There is no specific treatment for molluscum contagiosum, but various methods can be used to help manage the condition. These include topical treatments, such as antiviral creams or freezing the bumps with liquid nitrogen, and physical methods, such as scraping or cutting away the bumps. In some cases, the virus may recur even after treatment, so it's important to maintain good hygiene practices to prevent the spread of the infection.\n"
     ]
    }
   ],
   "source": [
    "# Prueba del modelo base\n",
    "\n",
    "#eval_prompt = \"\"\"Print hello world in python java and c\"\"\"\n",
    "\n",
    "eval_prompt = \"\"\"What is Molluscum contagiosum?\"\"\"\n",
    "\n",
    "# CUDA: Para programar directamente la GPU\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "base_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(base_model.generate(**model_input, max_new_tokens=512, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1dcf09-2684-40bf-a0ec-55e869ff04d3",
   "metadata": {},
   "source": [
    "## Fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e034c-452f-4701-9bd7-cb4f6c4b65ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a87f7ab-0d4e-42f7-8940-8f6fbdef119b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7073d1f6-0c96-48a4-9875-9c2592979d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\201902452\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\201902452\\Downloads\\wandb\\run-20240420_135731-35tv8npk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mrfat/mistral7b-instruct-medic-v0.2/runs/35tv8npk' target=\"_blank\">grateful-cosmos-2</a></strong> to <a href='https://wandb.ai/mrfat/mistral7b-instruct-medic-v0.2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mrfat/mistral7b-instruct-medic-v0.2' target=\"_blank\">https://wandb.ai/mrfat/mistral7b-instruct-medic-v0.2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mrfat/mistral7b-instruct-medic-v0.2/runs/35tv8npk' target=\"_blank\">https://wandb.ai/mrfat/mistral7b-instruct-medic-v0.2/runs/35tv8npk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log in to WandDB\n",
    "import wandb\n",
    "\n",
    "!wandb login b0ee138ef7cb51349541df5f648e2172d699101c\n",
    "\n",
    "run = wandb.init(\n",
    "    project='mistral7b-instruct-medic-v0.2',\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c6dafd-dc0b-4a82-b6de-38cfc980d557",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd50477-ee64-4ce0-a605-804556c2cf4d",
   "metadata": {},
   "source": [
    "### MEDIC DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15e50e2c-d53a-4446-a49c-41b60cae478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|█████████████████████████████████████████████████████████████████| 1.41k/1.41k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|███████████████████████████████████████████████████████████| 10.6M/10.6M [00:00<00:00, 15.0MB/s]\n",
      "Generating train split: 100%|█████████████████████████████████████████| 10000/10000 [00:00<00:00, 112113.89 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'instruction'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"medalpaca/medical_meadow_wikidoc\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd2f5f6a-8c3b-496b-9436-fc952e386f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATASET MEDIC\n",
    "\n",
    "def formatting_func(example):\n",
    "    text = f\"### The following is a medical question: \\n### Medical question: {example['input']} \\n### Question answer: {example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "833fe917-86c0-4994-a3a1-7d15c8a57b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbff2750-b7f4-4c6f-9099-3b90646b0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt_test(prompt):\n",
    "    return tokenizer(formatting_func(prompt))\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6482764-0eaa-466a-a0d8-881cfd8cc0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c173e164-b1a4-46e9-af9d-d4fafe69a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output', 'instruction'],\n",
      "    num_rows: 8000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "feb109dd-6d99-4bce-9e98-57ec2695d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output', 'instruction'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1adf739-7bba-47e8-b17b-9eb6c004fd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████| 8000/8000 [00:05<00:00, 1470.88 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1638.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset_test = train_dataset.map(generate_and_tokenize_prompt_test)\n",
    "tokenized_val_dataset_test = eval_dataset.map(generate_and_tokenize_prompt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f302b50-7567-4312-b55a-df72e5db2d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset_test, tokenized_val_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6f308-8294-41e4-bc29-003fc0812935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2fccf31-c083-404a-a24f-0fe9a44bb1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████| 8000/8000 [00:04<00:00, 1759.59 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1781.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "788b60f7-2ee0-49ba-bd03-23cb2cff42b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output', 'instruction', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 8000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input', 'output', 'instruction', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset)\n",
    "print(tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9581d7d8-9c4c-4f70-93bd-e6b682cc8a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7708b2-636f-47ac-82c5-781762961dc6",
   "metadata": {},
   "source": [
    "### LORA CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b3ead0a-3936-4da1-b61e-f5cc1cb923a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Fine-tunning with QLoRA y Supervised Fine Tunning (SFT)\n",
    "from peft import get_peft_model\n",
    "\n",
    "# Set LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Model with LoRA adapters added\n",
    "print(get_peft_model(base_model, peft_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "633a6c65-16b6-4bf1-acf0-06b9e281ffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\201902452\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters (Loading the trainer)\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"wandb\" #Default logging_dir = *output_dir/runs/CURRENT_DATETIME_HOSTNAME*\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer for fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,  # Specify the maximum sequence length here\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00e5189b-3c72-48bd-a941-16197c7655d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 33:20, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.791500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.322800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.820700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.471600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.860100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.795700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.728900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.247900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.460600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.525500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.821300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.318900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.507200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.102900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.126700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.697400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.997700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.828800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.420100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.829100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.525500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.306800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.630200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.714700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.577500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.917100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.551100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.493500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.453700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.651500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.086300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.593200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.931500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.394800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.876500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.753700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.458300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.340300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.978100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.493100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.198400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.829400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.824700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.584800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.769100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.771100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.841200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>2.548100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.260600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.351300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.544400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.877700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.557000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.945900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.996900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.629600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.619600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.470200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.699400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.589100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.973100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.322900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.384600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.313100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.198300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.793700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.560800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.355100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.788100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.754200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.856100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>2.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.609100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.666100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.756100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.626800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.647700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.833700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.475500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.726800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.293900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>1.468500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.678100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.468600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.287800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>1.431900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.698100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>1.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.820100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.838500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>1.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>1.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.339900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.989700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.522200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>1.173200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>1.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.590500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>1.673200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>1.495500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.484900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>1.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1.598500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.596800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>1.513200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.541200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.631400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>1.176900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>1.416500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.439100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>1.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.065400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>1.762900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>1.176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>1.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.963500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.054300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>1.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>1.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>1.781900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.577600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>1.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>1.603600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.797600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>1.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>1.212900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>1.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>1.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>1.434900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>1.622800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>1.558100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>1.338500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>1.563800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.411300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>1.567600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>1.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>1.288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>1.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.571900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.649100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.972300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>1.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>2.069500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.504200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>1.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>1.607700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>1.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>1.744700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.264700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>1.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>1.815700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>1.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>1.766300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>1.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>1.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>1.542300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>1.309700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>1.631500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.468300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\201902452\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\utils\\save_and_load.py:177: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▂▃▃▂▇▂▄▄▂▂█▄▄▁▃▅▄▂▄▆▄▁▁▂▃▅▃▂▃▄▅▃▄▂▃▆▂▂▁</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▇█▇▅▆▅▆▅▇▅▃▃▇▅▆▅▁▅▁▃▅▆▆▄▅▃▄▂▄▂▅▄▅▅▂▇▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>5526298558464000.0</td></tr><tr><td>train/epoch</td><td>0.03125</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/grad_norm</td><td>3.47482</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>1.4683</td></tr><tr><td>train_loss</td><td>1.4532</td></tr><tr><td>train_runtime</td><td>2013.1261</td></tr><tr><td>train_samples_per_second</td><td>0.124</td></tr><tr><td>train_steps_per_second</td><td>0.124</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-cosmos-2</strong> at: <a href='https://wandb.ai/mrfat/mistral7b-instruct-medic-v0.2/runs/35tv8npk' target=\"_blank\">https://wandb.ai/mrfat/mistral7b-instruct-medic-v0.2/runs/35tv8npk</a><br/> View project at: <a href='https://wandb.ai/mrfat/mistral7b-instruct-medic-v0.2' target=\"_blank\">https://wandb.ai/mrfat/mistral7b-instruct-medic-v0.2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240420_135731-35tv8npk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5416345600\n"
     ]
    }
   ],
   "source": [
    "###### START TRAIN ########\n",
    "\n",
    "# Initialize SFTTrainer (Wandb starts automatically when this is run)\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model (tuned_model = \"mistral-code-test1\" )\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# www.wandb.ai/<your-profile-name>/projects\n",
    "\n",
    "print(base_model.get_memory_footprint())\n",
    "\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como el train y el validation se van reduciendo, podemos afirmar que el modelo esta entrenando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4945af8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|████████████████████████████████████████████████████| 865M/865M [00:26<00:00, 33.1MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MrFat/mistral7b_medic/commit/3b8cfccb38c46c74231ebbe2c2dab1635f3811be', commit_message='Upload model', commit_description='', oid='3b8cfccb38c46c74231ebbe2c2dab1635f3811be', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.push_to_hub(\"MrFat/mistral7b_medic\")\n",
    "tokenizer.push_to_hub(\"MrFat/mistral7b_medic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b724f20-2e1b-43f5-9e13-b8f45c3bd2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06468f63-f446-4c03-a429-a2853999ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf = \"MrFat/mistral7b_medic\"\n",
    "\n",
    "tokenizer_hf = AutoTokenizer.from_pretrained(\n",
    "    model_hf,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer_hf.pad_token = tokenizer_hf.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1afe3cde-5fbb-4125-88e5-11a0203557bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n                    in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to\n                    `from_pretrained`. Check\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                    for more details.\n                    ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m base_model_hf \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_hf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/transformers/v2.9.1/main_classes/model.html#transformers.PreTrainedModel.generate\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#Use past key values?\u001b[39;00m\n\u001b[0;32m     10\u001b[0m base_model_hf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Nos interesa usar los parametros actualizados, no los viejos (cached)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3627\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3624\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3627\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3629\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3630\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:86\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m     device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     83\u001b[0m         key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_not_convert\n\u001b[0;32m     84\u001b[0m     }\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m            Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m            quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m            in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m            `from_pretrained`. Check\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m            https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m            for more details.\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m            \"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m         )\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \n                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n                    in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to\n                    `from_pretrained`. Check\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                    for more details.\n                    "
     ]
    }
   ],
   "source": [
    "base_model_hf = AutoModelForCausalLM.from_pretrained(\n",
    "    model_hf,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage = True,\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "# https://huggingface.co/transformers/v2.9.1/main_classes/model.html#transformers.PreTrainedModel.generate\n",
    "#Use past key values?\n",
    "base_model_hf.config.use_cache = False  # Nos interesa usar los parametros actualizados, no los viejos (cached)\n",
    "\n",
    "# Mimic the behaviour of the original model at inference?\n",
    "base_model_hf.config.pretraining_tp = 1 #1 = disable\n",
    "\n",
    "print(base_model_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83826361-beb0-4d0e-9204-295bc94b8d90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_model_hf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# CUDA: Para programar directamente la GPU\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model_input \u001b[38;5;241m=\u001b[39m tokenizer_hf(eval_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mbase_model_hf\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokenizer_hf\u001b[38;5;241m.\u001b[39mdecode(base_model_hf\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_input, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_model_hf' is not defined"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"What is Molluscum contagiosum?\"\"\"\n",
    "\n",
    "# CUDA: Para programar directamente la GPU\n",
    "model_input = tokenizer_hf(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "base_model_hf.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer_hf.decode(base_model_hf.generate(**model_input, max_new_tokens=512, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1893bd5a-5b12-4be2-b7ac-9e277ec4171b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128e3ac-cc40-4368-a7c4-c2bf96b93410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Después de hacer el save\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12737bb4-8284-4112-b619-1b975a071974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "    tokenized_output =  tokenizer(pompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    model_inputs = tokenized_output.to(\"cuda\")\n",
    "\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "    return decoded_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae5868-4ead-4c21-85fa-8495456d801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_response(\"### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.### Input:\\nThe first thing to know is that guacamole is a popular dip made from avocados, tomatoes, onions, and spices. It originated in Mexico, and is generally eaten as an appetizer or snack. Here are some simple steps: Choose 2 ripe avocados, about 2 cups Mash avocados in a large bowl using a fork or potato masher Add in 1-2 chopped tomatoes, salt, pepper, 1-2 garlic cloves, minced, 1-2 teaspoons fresh lime juice, and 1⁄4-1⁄2 cup chopped cilantro (optional). Let sit for about 10 minutes Taste, and add more salt, pepper, cilantro, or lime juice if needed Guacamole is usually served with tortilla chips. There are many variations, such as adding sour cream, diced vegetables, or more spicy hot peppers.\\n\\n### Response:\", merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309d5ef-bd08-42b8-8eef-92c045767137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3fbcd5a-918e-450c-95ba-d500bd03c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m tensorboard.main --logdir=resultados/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0019dcdc-8333-4641-a079-d643fefe0822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty VRAM\n",
    "del tokenizer_hf\n",
    "del pipeline\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74094edb-5d73-4870-99b5-e4767dd66cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.01s/it]\n",
      "C:\\Users\\201902452\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1363: UserWarning: Current model requires 536879104 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "################# A PARTIR DE AQUI, EL MODELO YA HA SIDO FINE-TUNEADO ########################\n",
    "\n",
    "# Inference test\n",
    "\n",
    "eval_prompt = \"\"\"What is Molluscum contagiosum?\"\"\"\n",
    "\n",
    "# Reload model in FP16 (Para cargarlo luego mas tarde)\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"MrFat/mistral7b_medic\", #HuggingFace upload\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49901d8-bd3a-42c0-b188-de96c0093f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26cc5207-02c6-409b-a81b-6d2294604387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.96s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_model_name = \"MrFat/mistral7b_medic\"\n",
    "adapter_model_name = \"mistral7b_medic\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    low_cpu_mem_usage = True,\n",
    "    device_map = device_map\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, adapter_model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46b2f31b-9716-4c99-93ab-71747006367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 6\u001b[0m     generated_code \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_code)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#outputs = model.generate(**model_input, max_new_tokens=100, return_dict_in_generate=True, output_scores=True)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#generated_token_ids = outputs.sequences\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#generated_text = tokenizer.decode(generated_token_ids[0], skip_spectial_tokens=True)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\peft_model.py:1365\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1364\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1365\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1367\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1576\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1559\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assisted_decoding(\n\u001b[0;32m   1560\u001b[0m         input_ids,\n\u001b[0;32m   1561\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1573\u001b[0m     )\n\u001b[0;32m   1574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[0;32m   1575\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1576\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[0;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:2494\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2491\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2494\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2498\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2502\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:1158\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1155\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1158\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1170\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1171\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:987\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    984\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_length)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 987\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_cache:\n\u001b[0;32m    990\u001b[0m     is_padding_right \u001b[38;5;241m=\u001b[39m attention_mask[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m!=\u001b[39m batch_size\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"What is Molluscum contagiosum?\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_code = tokenizer.decode(model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True)\n",
    "print(generated_code)\n",
    "\n",
    "#outputs = model.generate(**model_input, max_new_tokens=100, return_dict_in_generate=True, output_scores=True)\n",
    "#generated_token_ids = outputs.sequences\n",
    "#generated_text = tokenizer.decode(generated_token_ids[0], skip_spectial_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652cc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec730fbc-7b9a-4268-9900-36712043566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 16\u001b[0m\n\u001b[0;32m      6\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrograma en python una funcion para contar sumar los 5 primeros números\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m sequences \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(sequences[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:240\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1242\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1235\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1236\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1239\u001b[0m         )\n\u001b[0;32m   1240\u001b[0m     )\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1249\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1248\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1249\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1250\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1149\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1148\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1149\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1150\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:327\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1622\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1614\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1615\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1616\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1617\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1618\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1619\u001b[0m     )\n\u001b[0;32m   1621\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1622\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1637\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1638\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1639\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1640\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1645\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1646\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:2791\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2788\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2790\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2794\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2795\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2796\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2799\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:1158\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1155\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1158\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1170\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1171\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:1043\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1033\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1034\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1035\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1040\u001b[0m         use_cache,\n\u001b[0;32m   1041\u001b[0m     )\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:770\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[0;32m    768\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    769\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 770\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    771\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    773\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:179\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:504\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 504\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m     torch_result_dtype \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m active_adapter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Pipeline function from Transformers library to generate response based on the prompt\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistral7b_medic\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistral7b_medic\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "prompt = \"Programa en python una funcion para contar sumar los 5 primeros números\"\n",
    "\n",
    "sequences = pipe(\n",
    "    prompt,\n",
    "    do_sample = True,\n",
    "    max_new_tokens = 100,\n",
    "    temperature = 0.7,\n",
    "    top_k = 50,\n",
    "    top_p = 0.95,\n",
    "    num_return_sequences = 1,\n",
    ")\n",
    "print(sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf09885",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is Datacamp Career track?\"\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d61e2-5995-49a0-b8b2-5bbf1f86573f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e33182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [02:04<00:00, 41.58s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "adapter_model_name = \"mistral7b_medic\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "model = PeftModel.from_pretrained(model, adapter_model_name)\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"merged_adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62364f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fff63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6689dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6feed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-patient-query-finetune/checkpoint-500/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce047119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(query):\n",
    "    eval_prompt = \"\"\"Patient's Query:\\n\\n {} ###\\n\\n\"\"\".format(query)\n",
    "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = ft_model.generate(input_ids=model_input[\"input_ids\"].to(device),\n",
    "                           attention_mask=model_input[\"attention_mask\"], \n",
    "                           max_new_tokens=125, repetition_penalty=1.15)\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True).replace(eval_prompt, \"\")\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
