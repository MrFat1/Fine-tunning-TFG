{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a6f583-2990-41af-a203-ce7745438c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q git+https://github.com/huggingface/peft.git\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q trl\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q wandb -U\n",
    "!pip install -q -U matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b74619e4-24f4-440a-bc96-a9fc810c5c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n",
      "\n",
      "- `transformers` version: 4.40.0\n",
      "- Platform: Windows-10-10.0.22621-SP0\n",
      "- Python version: 3.11.9\n",
      "- Huggingface_hub version: 0.22.2\n",
      "- Safetensors version: 0.4.3\n",
      "- Accelerate version: 0.29.3\n",
      "- Accelerate config: \tnot found\n",
      "- PyTorch version (GPU?): 2.2.2+cu121 (True)\n",
      "- Tensorflow version (GPU?): not installed (NA)\n",
      "- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n",
      "- Jax version: not installed\n",
      "- JaxLib version: not installed\n",
      "- Using GPU in script?: <fill in>\n",
      "- Using distributed or parallel set-up in script?: <fill in>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!transformers-cli env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8d8796-5f4d-4aaa-9627-d029d2dd23b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Aug_15_22:09:35_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.140\n",
      "Build cuda_12.2.r12.2/compiler.33191640_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "394c68ab-9bb8-4094-a192-cb806492d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipywidgets\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "223b6592-c4ff-4ea5-945f-77f26886a478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4619387f3345d58f4256a929ccd8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Log in to the HugginFace Model Hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dcd0ceb-679e-4912-ad4c-058dfe4dacbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: unrecognized arguments: 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\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ebe4c-6fd8-4e6c-864a-8eba0cb31f60",
   "metadata": {},
   "source": [
    "## Prueba del modelo base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd1aa6e-9ccc-4a2a-8bb1-7133967827a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\201902452\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import notebook_login # Usaremos las herramientas de HuggingFace para el entrenamiento\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import accelerate\n",
    "import tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "273d8210-03da-47cd-8a09-89226167022d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\201902452\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\201902452\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Base Model\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load MitsralAi tokenizer for dataset formatting\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.padding_side = \"left\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd2caf-37b0-4bb7-975a-8c0f9128e84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c3afd44-8c39-4d4d-a730-f029ded9d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros del modelo\n",
    "\n",
    "# Final model name\n",
    "tuned_model = \"mistral7b_medic\"\n",
    "\n",
    "######### QLORA Params #############\n",
    "# (Para reducir el uso de memoria)\n",
    "\n",
    "# Estos valores dependen del dataset\n",
    "# The rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. \n",
    "# A higher rank will allow for more expressivity, but there is a compute tradeoff. (2^x)\n",
    "lora_r = 32\n",
    "\n",
    "# Scaling factor for the learned weights. The weight matrix is scaled by alpha/r, and thus a higher value for alpha assigns more weight to \n",
    "# the LoRA activations.\n",
    "lora_alpha = 64\n",
    "\n",
    "# NOTA: En el paper de QLoRA utiliza los valoes de r = 64 y alpha = 16, argumentando que estos valores generalizan bastante bien. Si queremos darle mas\n",
    "# importancia a la data fine-tuneada aumentamos los valores de alpha y si queremos mejor rendimiento disminuimos R.\n",
    "\n",
    "# Dropout probability\n",
    "# Durante el entranmiento, en cada epoch hay un {lora_dropout}% de que las neuronas se desactiven (para que trabaje mas)\n",
    "lora_dropout = 0.1\n",
    "\n",
    "####### BitsAndBytes param ###########\n",
    "\n",
    "#Activamos la reducción de precisión a 4-bit\n",
    "use_4bit = True\n",
    "\n",
    "# Parámetro para los modelos 4-bit\n",
    "bnb_4bit_compute_dtype = \"bfloat16\" #torch.float16 != torch.bfloat16\n",
    "\n",
    "# Tipo de cuantización (fp4 o nf4)\n",
    "# nf4 utiliza una distribución normal\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Nested quantization for 4-bit base models (double quantization)\n",
    "# Nos proporciona una mayor eficiencia de memoria sin sacrificiar rendimiento. Lo que hace es\n",
    "# realizar una segunda cuantización de los pesos ya cuantizados para ahorrar 0.4 bits/parametro.\n",
    "use_nested_quant = True\n",
    "\n",
    "####### Training Arguments param #########\n",
    "\n",
    "#Aqui se guardarán las predicciones y los checkpoints\n",
    "output_dir = \"./resultados\"\n",
    "\n",
    "# Número de epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for train\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 2\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2.5e-5\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "# The total number of training steps to perform.\n",
    "# Uso: Inicialmente a muchos steps y comprobamos a partir de que steps el modelo empieza a degradarse. Para evitar hacer muchos entrenamientos\n",
    "# en la proxima iteración empezamos desde un checkpoint.\n",
    "\n",
    "max_steps = 250\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 1\n",
    "\n",
    "###### Parámetros para SFT ########\n",
    "\n",
    "# Max sequence length\n",
    "max_seq_length = 512\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "# (En el código de Accelerate esta todo explicado)\n",
    "# device_map = {\"\": 0}\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a51365b6-a74a-403e-a8ef-d73f23ce6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load base model\n",
    "\n",
    "# Load with QLoRA config\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# https://huggingface.co/docs/transformers/main_classes/quantization\n",
    "\n",
    "# Con la librería Transformers podemos usar los algoritmos AWQ y GPTQ de cuantización y soporta\n",
    "# cuantizaciones de 4 y 8 bits. (Se pueden añadir más técnicas con la clase HfQuantizer)\n",
    "# En este caso cuantizaremos a 4-bit con el tipo NF4\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b0564fd-8b00-4b9a-a04c-f22b29e008d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|████████████████████████████████████████████████████████████████| 3/3 [03:03<00:00, 61.04s/it]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/model_doc/auto\n",
    "# https://huggingface.co/transformers/v2.9.1/main_classes/model.html\n",
    "\n",
    "# Utilizamos la arquitectura que viene ya incluida en el modelo\n",
    "# Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage = True,\n",
    "    device_map = device_map\n",
    ")\n",
    "\n",
    "# https://huggingface.co/transformers/v2.9.1/main_classes/model.html#transformers.PreTrainedModel.generate\n",
    "#Use past key values?\n",
    "base_model.config.use_cache = False  # Nos interesa usar los parametros actualizados, no los viejos (cached)\n",
    "\n",
    "# Mimic the behaviour of the original model at inference?\n",
    "base_model.config.pretraining_tp = 1 #1 = disable\n",
    "\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23888e13-7c19-4ed7-ba96-2fd2c788be99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "C:\\Users\\201902452\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a time traveler from the year 4055. Write a letter to your past self describing the future.м\n",
      "\n",
      "Dear Me,\n",
      "\n",
      "I know you're probably wondering why I'm writing to you from the future. Well, I'm here to tell you that you're about to make a huge mistake. You're about to go on a date with that guy you met at the bar last night. He seems nice enough, but he's actually a serial killer. I know you don't believe me, but trust me, he's going to kill you.\n",
      "\n",
      "I know you're probably thinking, \"But he seemed so nice! He bought me a drink and we had a great conversation.\" But trust me, he's not who he seems. He's going to take you back to his place and kill you. I know you're probably thinking, \"But he said he was a doctor and he seemed so smart.\" But trust me, he's not who he seems. He's going to take you back to his place and kill you.\n",
      "\n",
      "I know you're probably thinking, \"But he said he was a doctor and he seemed so smart.\" But trust me, he's not who he seems. He's going to take you back to his place and kill\n"
     ]
    }
   ],
   "source": [
    "# Prueba del modelo base\n",
    "\n",
    "#eval_prompt = \"\"\"Print hello world in python java and c\"\"\"\n",
    "\n",
    "eval_prompt = \"\"\"You're a time traveler from the year 4055. Write a letter to your past self describing the future.\"\"\"\n",
    "\n",
    "# CUDA: Para programar directamente la GPU\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "base_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(base_model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1dcf09-2684-40bf-a0ec-55e869ff04d3",
   "metadata": {},
   "source": [
    "## Fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e034c-452f-4701-9bd7-cb4f6c4b65ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a87f7ab-0d4e-42f7-8940-8f6fbdef119b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7073d1f6-0c96-48a4-9875-9c2592979d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\201902452\\.netrc\n",
      "wandb: Currently logged in as: mrfat. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\201902452\\Desktop\\wandb\\run-20240419_224035-pmen20l7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mrfat/mistral7b-instruct-medic/runs/pmen20l7' target=\"_blank\">northern-brook-1</a></strong> to <a href='https://wandb.ai/mrfat/mistral7b-instruct-medic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mrfat/mistral7b-instruct-medic' target=\"_blank\">https://wandb.ai/mrfat/mistral7b-instruct-medic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mrfat/mistral7b-instruct-medic/runs/pmen20l7' target=\"_blank\">https://wandb.ai/mrfat/mistral7b-instruct-medic/runs/pmen20l7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log in to WandDB\n",
    "import wandb\n",
    "\n",
    "!wandb login b0ee138ef7cb51349541df5f648e2172d699101c\n",
    "\n",
    "run = wandb.init(\n",
    "    project='mistral7b-instruct-medic',\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c6dafd-dc0b-4a82-b6de-38cfc980d557",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd50477-ee64-4ce0-a605-804556c2cf4d",
   "metadata": {},
   "source": [
    "### MEDIC DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15e50e2c-d53a-4446-a49c-41b60cae478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|█████████████████████████████████████████████████████████████████| 1.41k/1.41k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|███████████████████████████████████████████████████████████| 10.6M/10.6M [00:01<00:00, 6.26MB/s]\n",
      "Generating train split: 100%|██████████████████████████████████████████| 10000/10000 [00:00<00:00, 86845.78 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'instruction'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"medalpaca/medical_meadow_wikidoc\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd2f5f6a-8c3b-496b-9436-fc952e386f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATASET MEDIC\n",
    "\n",
    "def formatting_func(example):\n",
    "    text = f\"### The following is a medical question: \\n### Medical question: {example['input']} \\n### Question answer: {example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833fe917-86c0-4994-a3a1-7d15c8a57b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbff2750-b7f4-4c6f-9099-3b90646b0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt_test(prompt):\n",
    "    return tokenizer(formatting_func(prompt))\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6482764-0eaa-466a-a0d8-881cfd8cc0ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'train_test_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m      3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'train_test_split'"
     ]
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c173e164-b1a4-46e9-af9d-d4fafe69a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output', 'instruction'],\n",
      "    num_rows: 8000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feb109dd-6d99-4bce-9e98-57ec2695d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output', 'instruction'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1adf739-7bba-47e8-b17b-9eb6c004fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset_test = train_dataset.map(generate_and_tokenize_prompt_test)\n",
    "tokenized_val_dataset_test = eval_dataset.map(generate_and_tokenize_prompt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f302b50-7567-4312-b55a-df72e5db2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_lengths(tokenized_train_dataset_test, tokenized_val_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6f308-8294-41e4-bc29-003fc0812935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2fccf31-c083-404a-a24f-0fe9a44bb1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████| 8000/8000 [00:07<00:00, 1140.73 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1173.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "788b60f7-2ee0-49ba-bd03-23cb2cff42b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output', 'instruction', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 8000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input', 'output', 'instruction', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset)\n",
    "print(tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9581d7d8-9c4c-4f70-93bd-e6b682cc8a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7708b2-636f-47ac-82c5-781762961dc6",
   "metadata": {},
   "source": [
    "### LORA CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b3ead0a-3936-4da1-b61e-f5cc1cb923a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Fine-tunning with QLoRA y Supervised Fine Tunning (SFT)\n",
    "from peft import get_peft_model\n",
    "\n",
    "# Set LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Model with LoRA adapters added\n",
    "print(get_peft_model(base_model, peft_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "633a6c65-16b6-4bf1-acf0-06b9e281ffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters (Loading the trainer)\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"wandb\" #Default logging_dir = *output_dir/runs/CURRENT_DATETIME_HOSTNAME*\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer for fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,  # Specify the maximum sequence length here\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00e5189b-3c72-48bd-a941-16197c7655d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\201902452\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 23/250 21:53 < 3:56:41, 0.02 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.165300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.697300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.933400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.567900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.590200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.589800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.669600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.509800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.742400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.717600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.670700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.541800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.642100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.395900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.355200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.168100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m###### START TRAIN ########\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize SFTTrainer (Wandb starts automatically when this is run)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model (tuned_model = \"mistral-code-test1\" )\u001b[39;00m\n\u001b[0;32m      7\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(tuned_model)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m--> 361\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2209\u001b[0m ):\n\u001b[0;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3147\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2013\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###### START TRAIN ########\n",
    "\n",
    "# Initialize SFTTrainer (Wandb starts automatically when this is run)\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model (tuned_model = \"mistral-code-test1\" )\n",
    "trainer.model.save_pretrained(tuned_model)\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# www.wandb.ai/<your-profile-name>/projects\n",
    "\n",
    "print(base_model.get_memory_footprint())\n",
    "\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3fbcd5a-918e-450c-95ba-d500bd03c191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5416345600\n"
     ]
    }
   ],
   "source": [
    "# python -m tensorboard.main --logdir=resultados/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "875c8062-5bd9-45b2-84d9-e09905a5386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do a java function for getting the even leters in an array\n",
      "\n",
      "import java.util.Scanner;\n",
      "\n",
      "public class EvenLetters {\n",
      "    public static void main(String[] args) {\n",
      "        Scanner sc = new Scanner(System.in);\n",
      "        System.out.println(\"Enter the size of the array: \");\n",
      "        int size = sc.nextInt();\n",
      "        String[] arr = new String[size];\n",
      "        System.out.println(\"Enter the elements of the array: \");\n",
      "        for (int i = 0; i < size; i++) {\n",
      "            arr[i] = sc.next();\n",
      "        }\n",
      "        System.out.println(\"The even letters in the array are: \");\n",
      "        for (int i = 0; i < size; i++) {\n",
      "            if (arr[i].length() % 2 == 0) {\n",
      "                System.out.println(arr[i]);\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "################# A PARTIR DE AQUI, EL MODELO YA HA SIDO FINE-TUNEADO ########################\n",
    "\n",
    "# Inference test\n",
    "\n",
    "eval_prompt = \"\"\"Do a java function for getting the even leters in an array\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "base_model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_code = tokenizer.decode(base_model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True)\n",
    "print(generated_code)\n",
    "\n",
    "#outputs = model.generate(**model_input, max_new_tokens=100, return_dict_in_generate=True, output_scores=True)\n",
    "#generated_token_ids = outputs.sequences\n",
    "#generated_text = tokenizer.decode(generated_token_ids[0], skip_spectial_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec730fbc-7b9a-4268-9900-36712043566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline function from Transformers library to generate response based on the prompt\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = base_model,\n",
    "    tokenizer = tokenizer,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "prompt = \"Programa en python una funcion para contar sumar los 5 primeros números\"\n",
    "\n",
    "sequences = pipe(\n",
    "    prompt,\n",
    "    do_sample = True,\n",
    "    max_new_tokens = 100,\n",
    "    temperature = 0.7,\n",
    "    top_k = 50,\n",
    "    top_p = 0.95,\n",
    "    num_return_sequences = 1,\n",
    ")\n",
    "print(sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf09885",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is Datacamp Career track?\"\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login\n",
    "\n",
    "model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e33182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"mistralaiCode\" #Name of the model you will be pushing to huggingface model hub\n",
    "\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62364f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 (Para cargarlo luego mas tarde)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, #HuggingFace upload\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
